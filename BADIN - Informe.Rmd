---
title: "Tesina"
author: "Licenciatura en Estad√≠stica"
output:
  bookdown::pdf_document2:
    toc: false            # Desactiva el TOC autom√°tico
    number_sections: true # Numera secciones y subsecciones
    fig_caption: true     # Habilita numeraci√≥n de figuras y tablas
    latex_engine: xelatex
lang: es-ES
header-includes:
  - \usepackage{array}
  - \usepackage[table]{xcolor}
  - \usepackage{graphicx}
  - \usepackage{titlesec}
  - \titleformat{\paragraph}[block]{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
  - \titlespacing*{\paragraph}{0pt}{1ex plus .2ex minus .2ex}{1ex}  
  - \usepackage{setspace}
  - \doublespacing
  - \setlength{\parindent}{1.25cm}
  - \setlength{\parskip}{0pt}
  - \usepackage{xcolor}
  - \usepackage{caption}
  - \captionsetup[table]{name=Tabla}
  - \definecolor{mygray}{gray}{0.95}
  - |
      \DeclareCaptionFormat{greybox}{%
        \colorbox{mygray}{%
          \parbox{\dimexpr\linewidth-2\fboxsep}{\centering #1#2#3}%
        }%
      }
  - \captionsetup[figure]{format=greybox, labelfont=it, textfont=it, justification=centering, singlelinecheck=false}
  - \usepackage{fancyvrb}
  - \DefineVerbatimEnvironment{Highlighting}{Verbatim}{fontsize=\small,commandchars=\\\{\}}
  - \renewcommand{\tablename}{Tabla}
bibliography: referencias.bib
csl: apa.csl
link-citations: true  # convierte citas en links a la lista final
nocite: |
  @*
---

```{r results='hide', message=FALSE, echo=FALSE, warning=FALSE}
set.seed(1406)
library(showtext)
showtext_auto()
font_add_google(name="Noto Serif", family = "libre")
```

```{=tex}
\thispagestyle{empty}
\begin{center}
    \vspace*{0.5cm}
    \huge\textbf{An√°lisis de comentarios en redes sociales con latent Dirichlet allocation}
    
    \vspace{4cm}
    \includegraphics[width=0.3\textwidth]{logo_unr.png}
  
    \vspace{0cm}
    \large{Facultad de Ciencias Econ√≥micas y Estad√≠stica}
    
    \vspace{0cm}
    \large{Alumna: Alfonsina Badin}
    
    \vspace{0cm}
    \large{Director: Ignacio Evangelista}
    
    \vspace{0.5cm}
    \large{2025}
    
\end{center}
```

\clearpage
\pagenumbering{arabic}
\setcounter{page}{1}

\renewcommand{\contentsname}{√çndice}

\setcounter{tocdepth}{4}
\tableofcontents

\clearpage

\newpage

# Agradecimientos

\newpage

# Resumen

\newpage

# Introducci√≥n {#sec-introduccion}

A lo largo de los a√±os, las redes sociales se han convertido en el medio de comunicaci√≥n m√°s utilizado. Diariamente, los usuarios ingresan a ellas para conversar con otras personas, dar a conocer sus puntos de vista, compartir experiencias, informarse sobre las √∫ltimas noticias y entretenerse.

En el a√±o 2020, a ra√≠z de la pandemia mundial por COVID-19, hubo un tipo de contenido que se populariz√≥ con gran velocidad: el *streaming*.  El *streaming* es una combinaci√≥n de radio y televisi√≥n, que se transmite en vivo en una plataforma gratuita (YouTube o Twitch) y luego es publicado *On-Demand* para que el usuario pueda verlo fuera de su horario habitual si as√≠ lo desea. Por lo general el formato consta de un panel de conductores que charlan de t√≥picos comunes para la audiencia, generando debates interesantes con los que los usuarios se pueden llegar a identificar. Dentro de cada canal de *streaming* se transmiten distintos programas que tienen un perfil particular y objetivos diferentes ya que su contenido puede variar en entretenimiento, humor, chimentos, noticias, etc.  

Su popularidad se debe a que es un contenido gratuito, cercano y que genera una **comunidad** entre los conductores y sus oyentes, quienes no s√≥lo interact√∫an en vivo a trav√©s de un *chat* sino que dejan sus comentarios en otras redes sociales: Instagram, TikTok, YouTube, entre otros.

Al publicar los programas *On-Demand*, los canales de *streaming* acceden a un recurso valioso que refleja la opini√≥n de sus oyentes: los comentarios de YouTube. El an√°lisis de estos textos puede resultar en conclusiones interesantes para los canales ya que permite un entendimiento de los sentimientos que tiene su comunidad con respecto a cada contenido.

Para llevar a cabo este an√°lisis, el presente informe utiliza **latent Dirichlet allocation (LDA)**, una t√©cnica de aprendizaje autom√°tico no supervisado que clasifica autom√°ticamente los textos en diferentes categor√≠as o temas seg√∫n las caracter√≠sticas del corpus. Esta metodolog√≠a permite detectar patrones tem√°ticos recurrentes en grandes vol√∫menes de datos textuales, como los comentarios de YouTube, y extraer informaci√≥n valiosa.

LDA se basa en un modelo probabil√≠stico generativo y es especialmente √∫til para modelar datos discretos como textos. Es un modelo bayesiano jer√°rquico de tres niveles, en el cual cada documento (en este caso, cada comentario) se representa como una mezcla de temas, y cada tema se define como una mezcla de palabras. En este sentido, LDA no solo agrupa comentarios en funci√≥n de temas compartidos, sino que tambi√©n asigna probabilidades a cada palabra dentro de cada tema, proporcionando una representaci√≥n expl√≠cita y estructurada del contenido del corpus.

Esta capacidad de modelar documentos como combinaciones de m√∫ltiples temas lo hace particularmente adecuado para analizar la diversidad tem√°tica de los comentarios de YouTube. Por ejemplo, un comentario podr√≠a estar relacionado en un 60% con el tema "entretenimiento" y en un 40% con el tema "noticias", lo que permite identificar la interacci√≥n entre los intereses de los usuarios.

Esta tesina tiene por objeto de estudio la t√©cnica latent Dirichlet allocation. Se brinda una introducci√≥n al tema, abarcando las definiciones b√°sicas, sus componentes, sus variantes, sus ventajas y limitaciones y sus campos de aplicaci√≥n. Se incluyen tambi√©n los conceptos te√≥ricos necesarios para comprender la metodolog√≠a. Adicionalmente, se presenta la aplicaci√≥n del modelo en comentarios de redes sociales.

\newpage

# Objetivos

## Objetivo general

El objetivo general del presente proyecto es profundizar en el estudio y aplicaci√≥n del modelo latent Dirichlet allocation (LDA) para la identificaci√≥n de t√≥picos o categor√≠as.

## Objetivos espec√≠ficos

- Comprender las bases del procesamiento de lenguaje natural (NLP por sus siglas en ingl√©s) y las t√©cnicas de representaci√≥n computacional de texto.
- Mencionar los desaf√≠os y limitaciones que conlleva el ajuste de LDA y propuestas que han surgido para contrarrestar dichos inconvenientes.
- Aplicar LDA en un conjunto comentarios en espa√±ol (argentino) en la red social YouTube para comprender las opiniones de los oyentes.

\newpage

# Metodolog√≠a

## Machine learning

### Aprendizaje supervisado

### Aprendizaje no supervisado

\newpage

## Procesamiento de Lenguaje Natural (NLP)

El texto, tal como lo leemos y comprendemos los humanos, es rico en significado pero carece de una estructura intr√≠nsecamente num√©rica, que es la base del procesamiento en los algoritmos de aprendizaje autom√°tico. El procesamiento de lenguaje natural es un √°rea de investigaci√≥n dentro de la inform√°tica y la inteligencia artificial (IA) que se ocupa de procesar el lenguaje humano. Este procesamiento generalmente implica traducir el lenguaje natural en datos (n√∫meros) que una computadora puede usar para aprender sobre el mundo.

El proceso abarca tareas como la recolecci√≥n, limpieza, normalizaci√≥n y tokenizaci√≥n de texto, estas √∫ltimas dos son las responsables de que la informaci√≥n textual pueda ser estructurada de manera que los modelos la interpreten y analicen eficazmente. En la Figura \@ref(fig:diagrama1)  (@vajjala2020) se pueden observar los pasos que comprende el proceso.

```{r diagrama1, fig.cap="Etapas gen√©ricas en NLP", out.width="100%", fig.align='center',echo=FALSE, warning=FALSE, message=FALSE}
knitr::include_graphics("_imagenes/diagrama1.png")
```

Aunque no sea evidente, el procesamiento de lenguaje natural (NLP) aparece en el d√≠a a d√≠a de las personas. Tiene una amplia variedad de aplicaciones pr√°cticas, lo que lo convierte en una herramienta esencial en diversos campos. Por ejemplo:

- Mejoras en motores de b√∫squeda web.
- Correcci√≥n ortogr√°fica durante la b√∫squeda.
- Revisi√≥n gramatical y ortogr√°fica.
- Desarrollo de chatbots y asistentes virtuales.
- Generaci√≥n de √≠ndices y tablas de contenido.
- Filtrado de spam en correo electr√≥nico.
- Personalizaci√≥n en campa√±as de marketing.

Este amplio rango de usos subraya su importancia como puente entre los lenguajes humanos y las capacidades computacionales.

### Recolecci√≥n de datos

El primer paso esencial en cualquier proyecto de procesamiento de lenguaje natural es la recolecci√≥n de datos. Esta etapa determina la calidad y relevancia de los resultados que se obtendr√°n al aplicar t√©cnicas avanzadas como latent Dirichlet allocation (LDA). Existen diferentes enfoques para recopilar datos en NLP, dependiendo de los objetivos del an√°lisis y la disponibilidad de recursos. A continuaci√≥n, se describen las principales estrategias de recolecci√≥n de datos:

#### Uso de conjuntos de datos p√∫blicos {-}

Una opci√≥n inicial es buscar conjuntos de datos p√∫blicos que sean relevantes para la tarea espec√≠fica. Si el conjunto de datos es adecuado, se puede proceder directamente a construir y evaluar un modelo. Sin embargo, cuando no se encuentra un dataset que cumpla los requisitos, se debe considerar generar datos personalizados.

#### Obtenci√≥n de datos de internet {-}

Otra estrategia consiste en identificar fuentes relevantes de datos en internet, como foros de consumidores o plataformas de discusi√≥n donde se publiquen consultas. Estos datos pueden ser extra√≠dos autom√°ticamente a trav√©s de t√©cnicas de *web scraping* y, posteriormente, etiquetados manualmente por anotadores humanos.

#### Combinaci√≥n de fuentes de datos {-}

En la pr√°ctica, los conjuntos de datos suelen provenir de fuentes heterog√©neas, como *datasets* p√∫blicos, datos etiquetados manualmente y datos aumentados. Esta combinaci√≥n es especialmente √∫til para construir modelos en etapas iniciales, cuando los datos espec√≠ficos para un escenario particular son limitados.

En esta tesina se opt√≥ por hacer uso de una *API* de Google para obtener los datos de entrada para el an√°lisis, este proceso ser√° detallado en el apartado de \hyperref[sec-aplicacionpractica]{\underline{\text{aplicaci√≥n pr√°ctica}}}

Esta etapa es crucial para garantizar la disponibilidad de datos limpios y √∫tiles, que son la base para las siguientes etapas de procesamiento y modelado. Una vez que se cuenta con los datos recolectados, se procede al siguiente paso: la limpieza y el preprocesamiento de texto.

### Limpieza de texto

La extracci√≥n y limpieza de texto se refiere al proceso de extraer de los datos de entrada el texto sin procesar, eliminando toda la informaci√≥n no textual, como marcas, metadatos, etc., y convirtiendolo al formato de codificaci√≥n requerido. La extracci√≥n de texto es un paso est√°ndar en el manejo de datos y, usualmente, no emplea t√©cnicas espec√≠ficas de NLP. Sin embargo, es un paso importante que tiene implicaciones para todos los dem√°s aspectos. Adem√°s, tambi√©n puede ser la parte m√°s demandante en t√©rminos de tiempo dentro de un proyecto.

#### Pasos preliminares {-}

Previo a la limpieza del texto, es necesario llevar a cabo una serie de transformaciones b√°sicas que garanticen la homogeneidad del corpus. En primer lugar, se convierte todo el texto a min√∫sculas con el fin de evitar que palabras id√©nticas sean tratadas como distintas debido a diferencias en el uso de may√∫sculas, por ejemplo, ‚ÄúVideo‚Äù y ‚Äúvideo‚Äù. Otro paso fundamental consiste en la eliminaci√≥n de signos de puntuaci√≥n, caracteres especiales y espacios innecesarios, que no aportan informaci√≥n sem√°ntica al an√°lisis y pueden generar una dispersi√≥n artificial en el vocabulario. En esta etapa tambi√©n se contempla la reducci√≥n de espacios m√∫ltiples a un √∫nico espacio, asegurando as√≠ una identificaci√≥n m√°s precisa de las palabras.

Finalmente, se corrigen patrones de escritura caracter√≠sticos de entornos informales, como la repetici√≥n exagerada de letras para enfatizar una emoci√≥n (por ejemplo, ‚Äúamoo‚Äù en lugar de ‚Äúamo‚Äù o ‚Äúencantaa‚Äù en lugar de ‚Äúencanta‚Äù). Este tipo de modificaciones, frecuentes en comentarios de redes sociales, pueden inflar el vocabulario y dificultar la identificaci√≥n de t√©rminos representativos. La normalizaci√≥n de estas formas al est√°ndar correspondiente resulta clave para mejorar la coherencia interna del corpus y facilitar los pasos posteriores de an√°lisis.

#### Tokenizaci√≥n {-}

La tokenizaci√≥n es un paso fundamental en el procesamiento del lenguaje natural que tiene como objetivo transformar el texto libre en una secuencia de unidades discretas que puedan ser procesadas computacionalmente. Consiste en segmentar el texto en unidades m√≠nimas llamadas tokens, cuya definici√≥n depende de la granularidad aplicada en el proceso:

- A nivel de palabra: cada token corresponde a una palabra. Ejemplo: ‚ÄúMuy bueno el video‚Äù ‚Üí [‚ÄúMuy‚Äù, ‚Äúbueno‚Äù, ‚Äúel‚Äù, ‚Äúvideo‚Äù].
- A nivel de sub-palabra: cada token es un fragmento de una palabra. Ejemplo: ‚Äújugando‚Äù ‚Üí [‚Äújug‚Äù, ‚Äúando‚Äù].
- A nivel de car√°cter: cada token es un √∫nico car√°cter (letra, signo de puntuaci√≥n, n√∫mero, etc.). Ejemplo: ‚Äúsol‚Äù ‚Üí [‚Äús‚Äù, ‚Äúo‚Äù, ‚Äúl‚Äù].
- A nivel de oraci√≥n: cada token corresponde a una oraci√≥n completa. Ejemplo: ‚ÄúHoy llueve. Ma√±ana saldr√° el sol.‚Äù ‚Üí [‚ÄúHoy llueve.‚Äù, ‚ÄúMa√±ana saldr√° el sol.‚Äù].

#### Normalizaci√≥n de unicode {-}

Al limpiar texto, especialmente que se ha obtenido a trav√©s de Web Scrapping, es posible que se encuentren varios caracteres Unicode, incluidos s√≠mbolos, emojis y otros caracteres gr√°ficos, algunos ejemplos se muestran en la Figura \@ref(fig:diagrama2)  (@vajjala2020).

```{r diagrama2, fig.cap="Caracteres unicode y su representaci√≥n gr√°fica", out.width="100%", fig.align='center',echo=FALSE, warning=FALSE, message=FALSE}
knitr::include_graphics("_imagenes/diagrama2.png")
```

Para interpretar estos s√≠mbolos no textuales y caracteres especiales se utiliza la normalizaci√≥n de *unicode*, que convierte el texto en alguna forma de representaci√≥n binaria para ser almacenado en una computadora. Existen varios esquemas de codificaci√≥n, y la codificaci√≥n predeterminada puede variar seg√∫n el sistema operativo. 

El siguiente fragmento de c√≥digo en R ejemplifica la limpieza de emojis en texto. Utilizando expresiones regulares, se eliminan todos los caracteres que no pertenezcan al alfabeto en min√∫sculas.

```{r warning=FALSE, message=FALSE, size='small'}
comentarios <- c(
  "Me encant√≥ este video üòçüî•", "Lo volver√≠a a ver!!! üíØ‚úîÔ∏è",
  "Qu√© aburrido üò¥üò¥ no me gust√≥", "Muy bueno!!! üëçüëèüëè"
)
comentarios_limpios <- gsub("[^a-z√°√©√≠√≥√∫√± ]", " ", tolower(comentarios))
comentarios_limpios <- stringr::str_squish(comentarios_limpios)
comentarios_limpios
```

La salida es un conjunto de comentarios donde los emojis y otros s√≠mbolos fueron eliminados, manteniendo √∫nicamente el texto en min√∫sculas y sin caracteres no deseados.

#### Eliminaci√≥n de stopwords {-}

Los *stopwords* son palabras funcionales que, si bien cumplen un rol gramatical importante (como preposiciones, art√≠culos, pronombres...), no aportan contenido sem√°ntico relevante al an√°lisis del significado global del texto.

El objetivo de esta etapa es reducir el ruido ling√º√≠stico y centrar el an√°lisis en las palabras que efectivamente comunican informaci√≥n. Por ejemplo, en frases como `"gracias por el dato"` o `"esto es una joya"`, las palabras `por`, `el`, `es`, `una` son t√≠picamente consideradas *stopwords*, mientras que `gracias`, `dato`, `joya` son t√©rminos de inter√©s.

Para llevar a cabo esta tarea se utilizar√° el paquete `tm` (*Text mining*) que cuenta con un listado de *stopwords* (Tabla \@ref(tab:stopwords)) y funciones como `removeWords` que permite eliminarlas de un texto dado.

```{r stopwords, echo=FALSE, warning=FALSE, message=FALSE}
library(tm)
library(kableExtra)
library(dplyr)

palabras <- stopwords("es")
ncol <- 10
nrow <- 2
palabras <- palabras[1:(nrow * ncol)]
mat <- matrix(palabras, nrow = nrow, byrow = TRUE)
df <- as.data.frame(mat, stringsAsFactors = FALSE)
kable(df, format = "latex", booktabs = TRUE,
      caption = "Algunas stopwords del paquete",
      col.names = NULL) %>%
  kable_styling(latex_options = c("HOLD_position", "scale_down", "striped"))
```

```{r warning=FALSE, message=FALSE}
library(tm)
prueba <- c(
  "gracias por el dato",
  "esto es una joya"
)
removeWords(prueba,stopwords("es"))
```

#### Correcci√≥n ortogr√°fica {-}

La correcci√≥n de errores en la escritura es un aspecto fundamental del procesamiento del lenguaje natural ya que los textos pueden contener diferentes tipos de fallos que afectan su comprensi√≥n y coherencia. Los errores en la redacci√≥n pueden clasificarse en tres categor√≠as principales *(Moyotl-Hern√°ndez, 2016)*:

- Errores ortogr√°ficos: ocurren cuando una palabra escrita no existe dentro del idioma.
- Errores gramaticales: Se presentan cuando las palabras utilizadas existen en el idioma, pero no son correctas en el contexto de la oraci√≥n.
- Errores de estilo: se refieren a palabras redundantes, ambiguas o repetidas que afectan la claridad del texto.

En este sentido, un corrector ortogr√°fico tiene como objetivo identificar palabras mal escritas dentro de un texto y sugerir la opci√≥n m√°s adecuada a partir de un conjunto de t√©rminos v√°lidos en el idioma. 

#### Correcci√≥n ortogr√°fica empleando distancia de Levenshtein-Damerau {-}

La distancia de Levenshtein o distancia de edici√≥n es una medida utilizada para calcular la similitud entre palabras, se trata de un conteo de operaciones requeridas para convertir una cadena de caracteres (una palabra) en otra. Las operaciones de edici√≥n son:

- Intersecci√≥n de un caracter: hogr $\rightarrow$ hogar (agregar ‚Äòa‚Äô entre la ‚Äòh‚Äô y la ‚Äòg‚Äô).
- Eliminaci√≥n de un caracter: arggentina $\rightarrow$ argentina (eliminar la ‚Äòg‚Äô).
- Sustituci√≥n de un caracter por otro: numero $\rightarrow$ n√∫mero (reemplazar la ‚Äòu‚Äô por la ‚Äò√∫‚Äô).

Hay una generalizaci√≥n de esta medida que consideran el intercambio de dos caracteres como una operaci√≥n: la distancia de Levenshtein-Damerau. En este m√©todo, aparece:

- Transposici√≥n de un caracter por otro: paelta $\rightarrow$ paleta (intercambia el lugar de la ‚Äòe‚Äô y la ‚Äòl‚Äô).

El m√©todo de correcci√≥n basado en la distancia de Levenshtein-Damerau se fundamenta en la b√∫squeda de la palabra m√°s similar dentro de un diccionario para corregir una palabra mal escrita. Este procedimiento implica generar todas las posibles transformaciones de la palabra err√≥nea mediante las operaciones de edici√≥n. Luego, cada una de estas transformaciones se compara con las palabras existentes en el diccionario, y aquellas que coincidan se agregan a una lista de sugerencias. Finalmente, la mejor correcci√≥n ser√° aquella con la menor distancia a la palabra original.  

En la Tabla \@ref(tab:tablaburri) se muestran todas las palabras generadas a partir de la palabra err√≥nea `burri` con una sola operaci√≥n de edici√≥n. 

```{r tablaburri, echo=FALSE, warning=FALSE, message=FALSE, }
library(knitr)

tabla_op <- data.frame(
  Operaci√≥n = c("Eliminaci√≥n", "Inserci√≥n", "Sustituci√≥n", "Transposici√≥n"),
  `Palabras generadas` = c(
    "urri, brri, buri, bur",
    "aburri, bburri, cburri, ..., zburri, burris, burria, ..., burriz",
    "aurri, burrs, murri, ..., burro, burrq, ..., burrz",
    "ubrri, bruri, burr, burir"
  )
)

kable(tabla_op, format = "latex", booktabs = TRUE,
      caption = "Posibles transformaciones de la palabra burri con distancia de edici√≥n uno",
      col.names = c("Operaci√≥n", "Palabras generadas")) %>%
  kable_styling(latex_options = c("HOLD_position", "striped"))
```

El desaf√≠o radica en seleccionar la cantidad de operaciones admitidas para la correcci√≥n y cu√°l de estas opciones es la m√°s apropiada. En el siguiente c√≥digo se define un diccionario ficticio para este caso y se calcula la distancia L-D haciendo uso del paquete `stringdist` para cada palabra del mismo. En la Tabla \@ref(tab:tablaburri1) se observan los resultados.

```{r warning=FALSE, message=FALSE, results='hide'}
# Paquetes
library(stringdist)
library(dplyr)
# 1. Token a corregir
token <- "burri"
# 2. Diccionario ficticio 
diccionario <- c("burro", "burrito", "barri", "burla", "buri", 
                 "perro", "burris", "aburris", "aburro", "caballo")
# 3. Calculamos distancia DL
resultados <- tibble::tibble(
  candidato = diccionario,
  distancia = stringdist(token, diccionario, method = "dl")
) %>%
  arrange(distancia, candidato)
```

```{r tablaburri1, echo=FALSE, warning=FALSE, message=FALSE}
n <- nrow(resultados)
half <- ceiling(n/2)

resultados <- tibble::tibble(
  candidato1 = resultados$candidato[1:half],
  distancia1 = resultados$distancia[1:half],
  candidato2 = resultados$candidato[(half+1):n],
  distancia2 = resultados$distancia[(half+1):n]
)

kable(resultados, format = "latex", booktabs = TRUE,
      caption = paste0("Posibles correcciones para la palabra '", token, "'"),
      col.names = c("Candidato", "Distancia", "Candidato", "Distancia")) %>%
  kable_styling(latex_options = c("HOLD_position", "striped", "scale_down"))
```

Si bien es f√°cil identificar que `perro` o `caballo` no son buenas correcciones, observando el resultado de la distancia L-D se llega a la misma conclusi√≥n. Para este conjunto de datos y el ejemplo en particular, hay correcciones mejores en funci√≥n de este c√°lculo.

En s√≠ntesis, el uso de la distancia de Levenshtein-Damerau constituye una herramienta potente para la correcci√≥n ortogr√°fica, ya que permite identificar palabras candidatas a partir de criterios formales de similitud. No obstante, el desaf√≠o principal radica en establecer un umbral adecuado de tolerancia en la distancia: una tolerancia demasiado estricta puede dejar sin corregir errores evidentes, mientras que una tolerancia demasiado laxa puede introducir correcciones incorrectas. De igual modo, disponer de un diccionario amplio, representativo y ajustado al dominio del corpus resulta esencial para que las sugerencias tengan sentido ling√º√≠stico y contextual. En conjunto, ambos elementos, son determinantes para perfeccionar los procesos de correcci√≥n ortogr√°fica y garantizar resultados confiables en etapas posteriores del an√°lisis de texto.

#### Lematizaci√≥n {-}

La lematizaci√≥n es un proceso de normalizaci√≥n l√©xica en NLP cuyo objetivo es reducir las palabras a su forma can√≥nica o ‚Äúlema‚Äù. Al aplicar lematizaci√≥n:

- Se eliminan variaciones flexivas (tiempo verbal, n√∫mero, g√©nero).
- Se conserva el significado l√©xico de la palabra.
- Se mejora la coherencia del vocabulario al reducir distintas formas a un √∫nico t√©rmino.

El paquete `UDPipe` implementa modelos de *Universal Dependencies* que permiten realizar tokenizaci√≥n, etiquetado gramatical y lematizaci√≥n de textos en m√∫ltiples lenguas.

```{r message=FALSE, warning=FALSE, results='hide'}
library(udpipe)
modelo <- udpipe_download_model(language = "spanish-gsd")
udmodel <- udpipe_load_model(file = modelo$file_model)
texto <- c("Los estudiantes estaban estudiando en la biblioteca.")
anot <- udpipe_annotate(udmodel, x = texto, doc_id = 1)
```

```{r tablalematizacion, echo=FALSE, warning=FALSE, message=FALSE}
anot <- as.data.frame(anot)

udpipe_out <- data.frame(
  Token = anot$token,
  Lema  = anot$lemma,
  UPOS  = anot$upos
)
kable(udpipe_out, format = "latex", booktabs = TRUE,
      caption = "Salida de UDPipe: tokens, lemas y categor√≠as gramaticales") %>%
  kable_styling(latex_options = c("HOLD_position", "striped", "scale_down"))
```

En la Tabla \@ref(tab:tablalematizacion) se observan los resultados de la lematizaci√≥n con una oraci√≥n simple. El c√≥digo devuelve palabras separadas, los lemas correspondientes y el tipo de palabra procesada, entre ellas:

- DET: determinante.
- NOUN: sustantivo.
- VERB: verbo.
- ADP: adposici√≥n.
- PUNCT: puntuaci√≥n.

Estos son algunos de los tipos de palabras que el paquete `UDPipe` reconoce. Este etiquetado permite filtrar aquellas categor√≠as con mayor carga sem√°ntica (sustantivos, verbos, adjetivos, adverbios y nombres propios) y as√≠ disminuir la dimensionalidad del *input* del modelo.

### Representaci√≥n de texto

La extracci√≥n de caracter√≠sticas relevantes de un texto es un paso clave para su procesamiento ya que incluso el mejor modelo posible, con informaci√≥n pobre, devuelve pobres resultados. En este apartado se desarrollar√°n algunas t√©cnicas para transformar texto en una representaci√≥n num√©rica que pueda alimentar un algoritmo de Machine Learning correctamente.

\newpage 

# Aplicaci√≥n pr√°ctica {#sec-aplicacionpractica}

En este apartado se expone la aplicaci√≥n pr√°ctica de las t√©cnicas de procesamiento de lenguaje natural (NLP) y del modelo latent Dirichlet allocation (LDA) sobre la base de comentarios recolectados de YouTube. El objetivo es mostrar, paso a paso, c√≥mo los procedimientos te√≥ricos previamente descritos se implementan en un corpus real, abarcando desde la carga inicial de los datos hasta la construcci√≥n de la matriz documento‚Äìt√©rmino y el posterior modelado de t√≥picos.

## Recolecci√≥n de datos: comentarios de YouTube

Utilizando la API de YouTube brindada por el servicio de Google Cloud y con la herramienta App Scripts integrada en SpreadSheets (Google), se realiz√≥ una recolecci√≥n de informacion b√°sica de los videos publicados entre el 1 de enero del 2023 y el 21 de diciembre del 2024 de los canales m√°s vistos del pa√≠s: LuzuTV, Olga, Un Poco De Ruido, La Casa Streaming, Bondi Live, Vorterix, Ubana Play y Blender. Luego, filtrando s√≥lo los videos con duraci√≥n mayor a 10 minutos (para evitar incluir *shorts* o recortes), se utiliz√≥ la misma API conectada a R (versi√≥n 4.4.0) y el paquete `tuber` para la recolecci√≥n de comentarios de los videos identificados en la primer instancia (\hyperref[sec-anexoA]{\underline{\text{Anexo}}}). En la Tabla \@ref(tab:recoleccion1) se puede identificar la cantidad de informaci√≥n recolectada para cada canal de *streaming*.

```{r carga, results='hide', warning=FALSE, message=FALSE, echo=FALSE, } 
library(readxl)
library(janitor)
library(lubridate)
library(knitr)

canales <- c("LuzuTV", "Olga", "Un Poco De Ruido", "La Casa Streaming", 
             "Bondi Live", "Vorterix", "Urbana Play", "Blender")

fotos <- c("logos_streaming/luzu.png", "logos_streaming/olga.png", "logos_streaming/un_poco_de_ruido.png", 
           "logos_streaming/la_casa.png", "logos_streaming/bondi.png","logos_streaming/vorterix.png" , 
           "logos_streaming/urbana.png", "logos_streaming/blender.png")

Luzu <- read_excel("Bases/LuzuTV.xlsx") %>% clean_names() %>% distinct()
comentarios_luzu <- read_excel("Bases/comentarios_luzu.xlsx") %>% 
  filter(publishedAt <= as.Date("05/09/2025", "%d/%m/%Y")) %>% distinct()

Olga  <- read_excel("Bases/Olga.xlsx") %>% clean_names() %>% distinct()
comentarios_olga <- read_excel("Bases/comentarios_olga.xlsx") %>% 
  filter(publishedAt <= as.Date("05/09/2025", "%d/%m/%Y")) %>% distinct()

Unpoco  <- read_excel("Bases/Unpoco.xlsx") %>% clean_names() %>% distinct()
comentarios_unpoco <- read_excel("Bases/comentarios_unpoco.xlsx") %>% 
  filter(publishedAt <= as.Date("05/09/2025", "%d/%m/%Y")) %>% distinct()

Lacasa <- read_excel("Bases/Lacasa.xlsx") %>% clean_names() %>% distinct()
comentarios_lacasa <- read_excel("Bases/comentarios_lacasa.xlsx") %>% 
  filter(publishedAt <= as.Date("05/09/2025", "%d/%m/%Y")) %>% distinct()

Bondi <- read_excel("Bases/Bondi.xlsx") %>% clean_names() %>% distinct()
comentarios_bondi <- read_excel("Bases/comentarios_bondi.xlsx") %>% 
  filter(publishedAt <= as.Date("05/09/2025", "%d/%m/%Y")) %>% distinct()

Vorterix <- read_excel("Bases/Vorterix.xlsx") %>% clean_names() %>% distinct()
comentarios_vorterix <- read_excel("Bases/comentarios_vorterix.xlsx") %>% 
  filter(publishedAt <= as.Date("05/09/2025", "%d/%m/%Y")) %>% distinct()

Urbana <- read_excel("Bases/Urbana.xlsx") %>% clean_names() %>% distinct()
comentarios_urbana <- read_excel("Bases/comentarios_urbana.xlsx") %>% 
  filter(publishedAt <= as.Date("05/09/2025", "%d/%m/%Y")) %>% distinct()

Blender <- read_excel("Bases/Blender.xlsx") %>% clean_names() %>% distinct()
comentarios_blender <- read_excel("Bases/comentarios_blender.xlsx") %>% 
  filter(publishedAt <= as.Date("05/09/2025", "%d/%m/%Y")) %>% distinct()


diferencias <- c(
  sum(Luzu$comentarios_api)-nrow(comentarios_luzu),
  sum(Olga$comentarios_api)-nrow(comentarios_olga),
  sum(Unpoco$comentarios_api)-nrow(comentarios_unpoco),
  sum(Lacasa$comentarios_api)-nrow(comentarios_lacasa),
  sum(Bondi$comentarios_api)-nrow(comentarios_bondi),
  sum(Vorterix$comentarios_api)-nrow(comentarios_vorterix),
  sum(Urbana$comentarios_api)-nrow(comentarios_urbana),
  sum(Blender$comentarios_api)-nrow(comentarios_blender))
```

```{r recoleccion1, warning=FALSE, message=FALSE, echo=FALSE}

# Celda de imagen en LaTeX (usar SIEMPRE esto en PDF)
foto_latex <- sprintf("\\includegraphics[width=0.9cm]{%s}", fotos)

conteo_filas <- c(nrow(Luzu), nrow(Olga), nrow(Unpoco), nrow(Lacasa), 
                  nrow(Bondi), nrow(Vorterix), nrow(Urbana), nrow(Blender))

conteo_api   <- c(sum(Luzu$comentarios_api, na.rm = TRUE),
                  sum(Olga$comentarios_api,  na.rm = TRUE),
                  sum(Unpoco$comentarios_api, na.rm = TRUE), 
                  sum(Lacasa$comentarios_api, na.rm = TRUE), 
                  sum(Bondi$comentarios_api, na.rm = TRUE), 
                  sum(Vorterix$comentarios_api, na.rm = TRUE), 
                  sum(Urbana$comentarios_api, na.rm = TRUE), 
                  sum(Blender$comentarios_api, na.rm = TRUE)) 
                  
conteo_recolectados <- c(nrow(comentarios_luzu), 
                         nrow(comentarios_olga), 
                         nrow(comentarios_unpoco), 
                         nrow(comentarios_lacasa), 
                         nrow(comentarios_bondi), 
                         nrow(comentarios_vorterix), 
                         nrow(comentarios_urbana), 
                         nrow(comentarios_blender))

tabla <- data.frame(
  Foto = foto_latex,                    
  Canal = canales,
  Videos = conteo_filas,
  Comentarios = conteo_api,
  `Comentarios recolectados` = conteo_recolectados,
  check.names = FALSE
)

fila_total <- data.frame(
  Foto = "",
  Canal = "Total",
  Videos = sum(conteo_filas, na.rm = TRUE),
  Comentarios = sum(conteo_api, na.rm = TRUE),
  `Comentarios recolectados` = sum(conteo_recolectados, na.rm = TRUE),
  check.names = FALSE
)

tabla_final <- rbind(tabla, fila_total)

kable(tabla_final,
      format   = "latex",
      booktabs = TRUE,
      escape   = FALSE,
      align    = c("c","l","r","r","r"),
      caption  = "Informaci√≥n recolectada de cada canal",
      format.args = list(big.mark = ".", decimal.mark = ",")) %>%
  kable_styling(full_width = FALSE, position = "center", latex_options = "hold_position") %>%
  row_spec(0, extra_css = "vertical-align: middle !important;") %>%
  row_spec(nrow(tabla_final), background = "gray!15", bold = TRUE)
```

Es de destacar que la API de YouTube cuenta con un l√≠mite de consulta para las respuestas a comentarios: se retienen √∫nicamente los primeros 5 comentarios por cadena. Sin embargo, por estar dentro de un mismo "hilo", estos comentarios no cambian dr√°sticamente en su tem√°tica y no afecta los fines de esta investigaci√≥n. Adem√°s, el l√≠mite mencionado no permite tomar toda la informaci√≥n de una sola corrida, hubo que realizar la recolecci√≥n de a un d√≠a por canal usando dos conexiones en simult√°neo. Esta diferencia en la recolecci√≥n se mitiga filtrando los comentarios seg√∫n su fecha de publicaci√≥n previa al d√≠a 05/09/2025 (d√≠a de la primera recolecci√≥n). 

## Limpieza de texto

En total se recopilaron 1.692.025 comentarios a los que se les aplic√≥ una secuencia de preprocesamiento est√°ndar en NLP:

1. Min√∫sculas: conversi√≥n completa del texto a min√∫scula para evitar duplicidad por capitalizaci√≥n.
2. Colapso de vocales repetidas: reducci√≥n de alargamientos (‚Äúamoo‚Äù, ‚Äúencantaa‚Äù $\rightarrow$ ‚Äúamo‚Äù, ‚Äúencanta‚Äù), √∫til para comentarios informales.
3. Filtrado de caracteres: se conservaron √∫nicamente letras del alfabeto espa√±ol (incluyendo tildes y ‚Äú√±‚Äù) y espacios, eliminando n√∫meros, emojis y signos.
4. Normalizaci√≥n de espacios: supresi√≥n de espacios m√∫ltiples.
5. Eliminaci√≥n de stopwords: remoci√≥n de palabras que no aportan carga sem√°ntica.

Con el objetivo de asistir la correcci√≥n ortogr√°fica y preservar la jerga propia del dominio, se construy√≥ un diccionario combinado a partir de dos fuentes: por un lado, las 20.000 palabras m√°s frecuentes del propio corpus de comentarios y, por otro, un listado exhaustivo de palabras en espa√±ol extra√≠do de la base *Kaikki.org*, que recopila el contenido de *Wiktionary*. De esta manera, el diccionario resultante no solo incorpora modismos, nombres propios y expresiones caracter√≠sticas del corpus analizado, sino tambi√©n un repertorio amplio de formas v√°lidas del idioma espa√±ol, constituyendo as√≠ una base m√°s robusta para la etapa de correcci√≥n ortogr√°fica.

La estrategia propuesta para la correcci√≥n ortogr√°fica es identificar *tokens* fuera del diccionario y proponer correcciones mediante la distancia de Leveinshtein-Damerau. En caso de empate en la distancia m√≠nima, se seleccionar√° como correcci√≥n al candidato de mayor frecuencia en el diccionario, lo que favorece a las expresiones m√°s habituales del corpus. Si bien Damerau sugiere que la gran mayor√≠a de los errores se corrigen con una sola edici√≥n, en el presente trabajo se adopt√≥ un umbral m√°ximo de distancia menor o igual a 3. Esta decisi√≥n responde a la naturaleza del corpus, comentarios de YouTube, donde son habituales errores acumulados o repeticiones de caracteres. 

Para optimizar el proceso de correcci√≥n, se incorpor√≥ un filtro preliminar basado en bigramas de caracteres, entendidos como secuencias de dos caracteres consecutivos dentro de una palabra. El procedimiento consiste en comparar los bigramas del *token* err√≥neo con los de cada candidato del diccionario y conservar √∫nicamente aquellos que presentan al menos dos coincidencias. De este modo, se descartan antes del c√°lculo de la distancia de edici√≥n aquellas palabras que no comparten una estructura b√°sica con el *token* original. Por ejemplo, la palabra `burri` genera los bigramas `bu`, `ur`, `rr` y `ri`. El candidato `buro` supera el filtro, ya que comparte los bigramas `bu` y `ur`, mientras que `casa` no lo cumple, dado que ninguno de sus bigramas (`ca`, `as`, `sa`) coincide con los del t√©rmino analizado.

En la Tabla \@ref(tab:correccion) se pueden ver algunos ejemplos de la correcci√≥n realizada. De 38957 palabras con errores ortogr√°ficos, se obtuvo un conjunto de **X** palabras bien escritas. Este paso facilita la futura lematizaci√≥n y habilita la inserci√≥n del corpus al modelo.

Una vez realizada la correcci√≥n ortogr√°fica, el siguiente paso consiste en la lematizaci√≥n de las palabras. Este proceso busca reducir cada forma flexionada o derivada a su lema, es decir, la forma base que representa la unidad l√©xica. De esta manera, palabras como ‚Äúcaminando‚Äù, ‚Äúcamin√©‚Äù o ‚Äúcaminaremos‚Äù se normalizan en el lema ‚Äúcaminar‚Äù. La lematizaci√≥n permite disminuir la dispersi√≥n del vocabulario y concentrar la informaci√≥n en t√©rminos que capturan el significado esencial, facilitando as√≠ la construcci√≥n de una matriz documento‚Äìt√©rmino m√°s consistente y adecuada para el modelado posterior con LDA.

En la Tabla \@ref(tab:lema) se pueden ver los lemas de las palabras corregidas en la Tabla \@ref(tab:correccion). 

## Representaci√≥n de texto

\newpage

# Bibliograf√≠a

::: {#refs}
:::

\newpage

# Anexos

## Anexo: Recolecci√≥n de datos: comentarios de YouTube {#sec-anexoA}

A continuaci√≥n se detalla el c√≥digo con el que se tom√≥ la informaci√≥n b√°sica de cada video desde Google Apps Script. Para cada canal se configur√≥ su ID y nombre.

- LuzuTV: UCTHaNTsP7hsVgBxARZTuajw
- Olga: UC7mJ2EDXFomeDIRFu5FtEbA
- Un poco de ruido: UCg6kTB4vw1XYFBR4TtHaBuQ
- La casa streaming: UC4u0BhsSi33PS20_1JHiC5A
- Bondi: UCnZidingmuqNkaT9Wm64Xxg
- Vorterix: UCvCTWHCbBC0b9UIeLeNs8ug
- Ubana Play: UCC1kfsMJko54AqxtcFECt-A
- BLender: UC6pJGaMdx5Ter_8zYbLoRgA

```{r results='hide', eval=FALSE, warning=FALSE, message=FALSE}
/*************** CONFIG ****************/
const CHANNEL_ID = 'id_canal'; 
const SHEET_VIDEOS = 'nombre_canal';

const START_DATE_ISO = '2023-01-01T00:00:00Z';
const END_DATE_ISO   = '2024-12-22T00:00:00Z';

// Control de timeout
const PAGES_PER_RUN = 8;             
const MINUTES_BETWEEN_RUNS = 1;
const SAFETY_MS = 5 * 60 * 1000;
const ROW_BUFFER = 200;
/****************************************/

function startVideoCrawl() {
  // limpiar triggers del batch
  ScriptApp.getProjectTriggers()
    .filter(t => t.getHandlerFunction() === 'crawlVideosBatch')
    .forEach(t => ScriptApp.deleteTrigger(t));

  const ch = YouTube.Channels.list('snippet,statistics,contentDetails', { id: CHANNEL_ID });
  if (!ch.items || !ch.items.length) throw new Error('Canal no encontrado');
  const uploadsId   = ch.items[0].contentDetails.relatedPlaylists.uploads;
  const channelName = ch.items[0].snippet.title;
  const subs        = ch.items[0].statistics.subscriberCount;

  const ss = SpreadsheetApp.getActiveSpreadsheet();
  let sh = ss.getSheetByName(SHEET_VIDEOS);
  if (!sh) sh = ss.insertSheet(SHEET_VIDEOS); else sh.clear();
  sh.appendRow([
    'ChannelId','Canal','Suscriptores',
    'VideoId','T√≠tulo','URL',
    'Duraci√≥n ISO','Segundos','Fecha Publicaci√≥n (UTC)',
    'Vistas','Likes','Comentarios(API)','Tipo live','Idioma'
  ]);

  const props = PropertiesService.getScriptProperties();
  props.deleteAllProperties();
  props.setProperties({
    'v.uploadsId': uploadsId,
    'v.pageToken': '',
    'v.channelName': channelName,
    'v.subs': String(subs),
    'v.startISO': START_DATE_ISO,
    'v.endISO': END_DATE_ISO,
    'v.excludeShorts': String(EXCLUDE_SHORTS),
    'v.excludeLives': String(EXCLUDE_LIVES)
  }, true);

  Logger.log('startVideoCrawl OK. Canal=' + channelName);
  crawlVideosBatch(); // primer lote
}

function crawlVideosBatch() {
  const props = PropertiesService.getScriptProperties();
  const uploadsId    = props.getProperty('v.uploadsId');
  if (!uploadsId) { Logger.log('ERROR: sin estado. Ejecut√° startVideoCrawl().'); return; }
  let pageToken      = props.getProperty('v.pageToken') || '';
  const channelName  = props.getProperty('v.channelName');
  const subs         = props.getProperty('v.subs');
  const startISO     = props.getProperty('v.startISO');
  const endISO       = props.getProperty('v.endISO');
  const excludeShorts= props.getProperty('v.excludeShorts') === 'true';
  const excludeLives = props.getProperty('v.excludeLives')  === 'true';

  const startMs = Date.parse(startISO);
  const endMs   = Date.parse(endISO);

  const ss = SpreadsheetApp.getActiveSpreadsheet();
  const sh = ss.getSheetByName(SHEET_VIDEOS);
  const t0 = Date.now();

  let pages = 0, rowsWritten = 0;

  while (pages < PAGES_PER_RUN) {
    if ((Date.now() - t0) > SAFETY_MS) break; // margen anti-timeout

    const pl = YouTube.PlaylistItems.list('contentDetails', {
      playlistId: uploadsId, maxResults: 50, pageToken: pageToken || undefined
    });
    const items = pl.items || [];
    Logger.log(`Playlist page items=${items.length} token=${pageToken||'(first)'}`);
    if (!items.length) { finishVideosCrawl_(); return; }

    const ids = items.map(x => x.contentDetails.videoId).join(',');
    const vd  = YouTube.Videos.list('snippet,statistics,contentDetails', { id: ids });

    let pageMin = +Infinity, pageMax = -Infinity;
    const rows = [];

    (vd.items || []).forEach(v => {
      const sn = v.snippet, st = v.statistics, cd = v.contentDetails;
      const pubMs = Date.parse(sn.publishedAt); // UTC
      if (pubMs < pageMin) pageMin = pubMs;
      if (pubMs > pageMax) pageMax = pubMs;

      // Si cae dentro del rango, aplico filtros y guardo
      if (pubMs >= startMs && pubMs < endMs) {
        const secs = iso8601ToSeconds_(cd.duration);
        const liveType = sn.liveBroadcastContent || 'none';
        if (excludeShorts && secs < 60) return;
        if (excludeLives  && liveType !== 'none') return;

        rows.push([
          CHANNEL_ID, channelName, subs,
          v.id, sn.title, 'https://www.youtube.com/watch?v=' + v.id,
          cd.duration, secs, sn.publishedAt,
          Number(st.viewCount||0), Number(st.likeCount||0), Number(st.commentCount||0),
          liveType, sn.defaultAudioLanguage || ''
        ]);
      }
    });

    if (rows.length) { appendBatch_(sh, rows); rowsWritten += rows.length; }

    if (pageMax < startMs) {
      Logger.log(`FIN por corte temprano: llegamos antes de ${START_DATE_ISO}. P√°ginas=${pages+1}, filas acumuladas=${rowsWritten}.`);
      finishVideosCrawl_();
      return;
    }

    pageToken = pl.nextPageToken || '';
    pages++;
    if (!pageToken) {
      Logger.log(`FIN: p√°ginas=${pages}, filas=${rowsWritten}.`);
      finishVideosCrawl_();
      return;
    }
  }

  props.setProperty('v.pageToken', pageToken);
  SpreadsheetApp.flush();
  Logger.log(`Continuar√°: p√°ginas=${pages}, filas=${rowsWritten}.`);
  ScriptApp.newTrigger('crawlVideosBatch').timeBased().after(MINUTES_BETWEEN_RUNS * 60 * 1000).create();
}

function finishVideosCrawl_() {
  ScriptApp.getProjectTriggers()
    .filter(t => t.getHandlerFunction() === 'crawlVideosBatch')
    .forEach(t => ScriptApp.deleteTrigger(t));
  PropertiesService.getScriptProperties().deleteAllProperties();
  Logger.log('Crawl de VIDEOS finalizado.');
}

function appendBatch_(sheet, rows2D) {
  const needed = rows2D.length;
  const last   = sheet.getLastRow();
  const max    = sheet.getMaxRows();
  const cols   = rows2D[0].length;

  const free = max - last;
  if (free < needed) {
    const toAdd = (needed - free) + ROW_BUFFER;
    sheet.insertRowsAfter(max, toAdd);
  }
  sheet.getRange(last + 1, 1, needed, cols).setValues(rows2D);
}

function iso8601ToSeconds_(dur) {
  const m = dur && dur.match(/PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/);
  if (!m) return 0;
  const h = parseInt(m[1]||'0',10), mi = parseInt(m[2]||'0',10), s = parseInt(m[3]||'0',10);
  return h*3600 + mi*60 + s;
}
```

Mediante R, se hizo uso del siguiente c√≥digo para recolectar los comentarios de cada video captado en el paso anterior. Notar que `cliend_id` y `client_secret` son las credenciales utilizadas para conectar al servicio de API de Google y es √∫nico por proyecto y por usuario.

```{r results='hide', eval=FALSE, warning=FALSE, message=FALSE}
library(tuber)
library(dplyr)
library(purrr)
library(stringr)
library(tidyr)
library(readr)
library(readxl)
library(janitor)

canal <- read_excel("canal.xlsx") %>%
  clean_names() %>%
  distinct()

yt_oauth(
  app_id     = client_id,
  app_secret = client_secret,
  scope      = "ssl",   # <-- no pongas la URL; usa "ssl"
  token      = ""
)

video_ids <- canal$video_id

comentarios_canal <- data.frame()

for (i in 1:nrow(canal)) { 
  all_comments <- get_all_comments(video_id = video_ids[i]) 
  comentarios_canal <- rbind(comentarios_canal, all_comments)
}
```

